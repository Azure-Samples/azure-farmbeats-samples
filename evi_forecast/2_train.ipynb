{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train EVI Forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Third party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Local  imports\n",
    "from utils.config import farmbeats_config\n",
    "from utils.constants import CONSTANTS\n",
    "from utils.ard_util import ard_preprocess\n",
    "from utils.satellite_util import SatelliteUtil\n",
    "from utils.weather_util import WeatherUtil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Satellite and Weatther Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/home/temp/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load satellite data  local paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_links = pd.read_csv(CONSTANTS[\"sat_file_paths\"])\n",
    "sat_links[\"fileExist\"] = sat_links.filePath.apply(os.path.exists)\n",
    "sat_links.head()\n",
    "\n",
    "# TODO: Check fileExist is True for all rows and raise error  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of weather parameter used in model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_parms = [\n",
    "    'airTempMax-F', \n",
    "    'airTempMin-F', \n",
    "    'cloudCover-%', \n",
    "    'dewPoint-F', \n",
    "    'dewPointMax-F', \n",
    "    'dewPointMin-F', \n",
    "    'iceAccPeriod-in', \n",
    "    'liquidAccPeriod-in', \n",
    "    'longWaveRadiationAvg-W/m^2', \n",
    "    'petPeriod-in', \n",
    "    'precipitation-in', \n",
    "    'relativeHumidity-%', \n",
    "    'relativeHumidityMax-%', \n",
    "    'relativeHumidityMin-%', \n",
    "    'shortWaveRadiationAvg-W/m^2', \n",
    "    'snowAccPeriod-in', \n",
    "    'sunshineDuration-hours', \n",
    "    'temperature-F', \n",
    "    'windSpeed-mph', \n",
    "    'windSpeed2mAvg-mph', \n",
    "    'windSpeed2mMax-mph', \n",
    "    'windSpeed2mMin-mph', \n",
    "    'windSpeedMax-mph', \n",
    "    'windSpeedMin-mph\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Train and Validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine satellite file paths and weather file per boundary\n",
    "\n",
    "trainval = (\n",
    "    sat_links.drop_duplicates([\"boundaryId\", \"fileExist\"])\n",
    "    .groupby([\"boundaryId\"])[\"fileExist\"]\n",
    "    .agg({\"count\"})\n",
    "    .reset_index()\n",
    "    .query(\"count == 1\")\n",
    "    .drop([\"count\"], axis=1)\n",
    ")\n",
    "\n",
    "\n",
    "# Check for weather file exists or not\n",
    "trainval[\"w_exists\"] = (os.path.join(root_dir, trainval[\"boundaryId\"] + \"_historical.csv\")).apply(\n",
    "    os.path.exists\n",
    ")\n",
    "\n",
    "trainval = trainval.query(\"w_exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and validation sets in 80% and 20% respectively\n",
    "np.random.seed(10)\n",
    "trainval[\"trainval\"] = np.where(\n",
    "    np.random.uniform(0, 1, trainval.shape[0]) < 0.8, \"Train\", \"Val\"\n",
    ")\n",
    "\n",
    "# TODO: Tobe removed once no_boundaries are more than 1\n",
    "trainval = pd.concat([trainval]*2, ignore_index=True)\n",
    "trainval.loc[1,'trainval'] = 'Val'\n",
    "trainval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get weather statistics for Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean and standard deviation of training data weather parameters for normalization\n",
    "w_stats = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(os.path.join(root_dir, trainval[\"boundaryId\"] + \"_historical.csv\"))\n",
    "        for x in trainval.query('trainval == \"Train\"').boundaryId.values\n",
    "    ],\n",
    "    axis=0,\n",
    ")[weather_parms].agg({\"mean\", \"std\"})\n",
    "weather_mean = w_stats.filter(like=\"mean\", axis=0)[w_parms].values\n",
    "weather_std = w_stats.filter(like=\"std\", axis=0)[w_parms].values\n",
    "\n",
    "# Save weather parameters normalization stats\n",
    "os.makedirs(os.path.dirname(CONSTANTS[\"w_pkl\"]), exist_ok=True)\n",
    "with open(CONSTANTS[\"w_pkl\"], \"wb+\") as f:\n",
    "    pickle.dump([weather_parms, weather_mean, weather_std], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ARD(boundaryId):\n",
    "    # function for preparing Analysis Ready Dataset\n",
    "    # intended for use in _2_build_model.py\n",
    "    \n",
    "    boundary_id_sat_links = sat_links.query(\n",
    "        'boundaryId == @boundaryId'\n",
    "    )\n",
    "     \n",
    "    # in reading w_df, if error occurs with farm_code, change it to field_id\n",
    "    w_df = pd.read_csv(os.path.join(root_dir, trainval[\"boundaryId\"] + \"_historical.csv\"))\n",
    "    \n",
    "    da_pc = ard_preprocess(\n",
    "        sat_links1=boundary_id_sat_links,\n",
    "        w_df=w_df,\n",
    "        sat_res_x=CONSTANTS[\"sat_res_x_model\"],\n",
    "        var_name=CONSTANTS[\"var_name\"],\n",
    "        interp_date_start=CONSTANTS[\"interp_date_start\"],\n",
    "        interp_date_end=CONSTANTS[\"interp_date_end\"],\n",
    "        w_parms=w_parms,\n",
    "        input_days=CONSTANTS[\"input_days\"],\n",
    "        output_days=CONSTANTS[\"output_days\"],\n",
    "        ref_tm=CONSTANTS[\"ref_tm_model\"],\n",
    "        w_mn=w_mn,\n",
    "        w_sd=w_sd,\n",
    "    )\n",
    "    return da_pc.query(\n",
    "        \"nan_input_evi and nan_input_w and nan_output_evi and nan_output_w and input_evi_le1 and output_evi_le1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Analysis Ready Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get analysis ready dataset\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "with ThreadPoolExecutor(max_workers=100) as executor:\n",
    "    ards_fetch = [executor.submit(get_ARD, x) for x in trainval.boundaryId.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ards_fetch[0].result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat(\n",
    "    [\n",
    "        ards_fetch[x]\n",
    "        .result()\n",
    "        .assign(\n",
    "            boundary_code=trainval.boundaryId.values[x], trainval=trainval.trainval.values[x]\n",
    "        )\n",
    "        for x in range(len(trainval.boundaryId.values))\n",
    "        if ards_fetch[x].exception() == None\n",
    "    ],\n",
    "    axis=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data.query('trainval == \"Train\"')\n",
    "data_val = data.query('trainval == \"Val\"')\n",
    "\n",
    "# Prepare train and validation tensors\n",
    "# converting list variables in ARD DataFrame to numpy array (tensors)\n",
    "X_train = [\n",
    "    np.array(data_train.input_evi.to_list()),\n",
    "    np.array(data_train.input_weather.to_list()),\n",
    "    np.array(data_train.forecast_weather.to_list()),\n",
    "]\n",
    "Y_train = np.array(data_train.output_evi.to_list())\n",
    "\n",
    "\n",
    "X_val = [\n",
    "    np.array(data_val.input_evi.to_list()),\n",
    "    np.array(data_val.input_weather.to_list()),\n",
    "    np.array(data_val.forecast_weather.to_list()),\n",
    "]\n",
    "Y_val = np.array(data_val.output_evi.to_list())\n",
    "\n",
    "# Save Analysis Ready Dataset (ARD)\n",
    "os.makedirs(os.path.dirname(CONSTANTS[\"ardpkl\"]), exist_ok=True)\n",
    "with open(CONSTANTS[\"ardpkl\"], \"wb\") as f:\n",
    "    pickle.dump(da_fin, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_weather, x, y, z):\n",
    "    \"\"\"\n",
    "    Model architecture\n",
    "    \"\"\"\n",
    "    # intended for use in _2_build_model.py\n",
    "    # Define the tensors for the three input images\n",
    "    evi_input = keras.Input((CONSTANTS[\"input_days\"], 1), name=\"evi_input\")\n",
    "    weather_input = keras.Input(\n",
    "        (CONSTANTS[\"input_days\"], input_weather), name=\"weather_input\"\n",
    "    )\n",
    "    forecast_input = keras.Input(\n",
    "        (CONSTANTS[\"output_days\"], input_weather), name=\"forecast_input\"\n",
    "    )\n",
    "\n",
    "    dense_1 = keras.layers.LSTM(\n",
    "        x, activation=\"relu\", name=\"DeNse_1\", dropout=0.1, recurrent_dropout=0.1\n",
    "    )(evi_input)\n",
    "    dense_2 = keras.layers.LSTM(\n",
    "        y, activation=\"relu\", name=\"DeNse_2\", dropout=0.1, recurrent_dropout=0.1\n",
    "    )(weather_input)\n",
    "    dense_3 = keras.layers.LSTM(\n",
    "        z,\n",
    "        activation=\"relu\",\n",
    "        name=\"lstm_1\",\n",
    "        return_sequences=True,\n",
    "        dropout=0.1,\n",
    "        recurrent_dropout=0.1,\n",
    "    )(forecast_input)\n",
    "\n",
    "    dense_12 = keras.layers.concatenate(axis=-1, inputs=[dense_1, dense_2])\n",
    "    dense_12_1 = keras.layers.RepeatVector(10)(dense_12)\n",
    "    dense_123 = keras.layers.concatenate(axis=-1, inputs=[dense_12_1, dense_3])\n",
    "    prediction = keras.layers.LSTM(\n",
    "        1, activation=\"relu\", name=\"lstm_2\", return_sequences=True\n",
    "    )(dense_123)\n",
    "    # Connect the inputs with the outputs\n",
    "    finnet = keras.Model(\n",
    "        inputs=[evi_input, weather_input, forecast_input], outputs=prediction\n",
    "    )\n",
    "    # return the model\n",
    "    return finnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(len(w_parms), 100, 100, 100)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"mse\"])\n",
    "# Model run\n",
    "training_history = model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    epochs=20,\n",
    "    verbose=0,\n",
    "    validation_data=(X_val, Y_val),\n",
    "    callbacks=[],\n",
    "    batch_size=1000,\n",
    ")\n",
    "val_pred = model.predict(X_val)\n",
    "# Save model to h5 format\n",
    "tf.keras.models.save_model(\n",
    "    model, filepath= CONSTANTS[\"modelh5\"], save_format=\"h5\", overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize model error as function of forecast days\n",
    "err_pred = Y_val[:, :, 0] - val_pred[:, :, 0]\n",
    "err_base = -Y_val[:, :, 0] + X_val[0][:, -1, 0][:, np.newaxis]\n",
    "df_err_mn = pd.DataFrame(\n",
    "    {\n",
    "        \"Day_i_vs_Day_0\": np.sqrt(np.mean(err_base ** 2, axis=0)),\n",
    "        \"Predicted_vs_Actual\": np.sqrt(np.mean(err_pred ** 2, axis=0)),\n",
    "        \"Day\": 1 + np.arange(CONSTANTS[\"output_days\"]),\n",
    "    }\n",
    ").set_index([\"Day\"])\n",
    "df_err_mn.plot()\n",
    "plt.suptitle(\n",
    "    \"RMSE plot comparing Day i to Day 0 vs Day i to ANN model prediction\\n Validation RMSE: \"\n",
    "    + str(np.round(np.sqrt(training_history.history[\"val_mse\"][-1]), 4))\n",
    ")\n",
    "plt.savefig(CONSTANTS[\"model_result_png\"])"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}