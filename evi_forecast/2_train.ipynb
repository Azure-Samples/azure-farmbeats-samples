{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Copyright (c) Microsoft Corporation. All rights reserved.\n",
        "\n",
        "Licensed under the MIT License."
      ],
      "metadata": {},
      "id": "da881206"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train EVI Forecast"
      ],
      "metadata": {},
      "id": "ec8b6cf5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries"
      ],
      "metadata": {},
      "id": "e1870dbe"
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import pickle\n",
        "import sys\n",
        "from datetime import datetime\n",
        "\n",
        "# Third party imports\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Local  imports\n",
        "from utils.config import farmbeats_config\n",
        "from utils.constants import CONSTANTS\n",
        "from utils.ard_util import ard_preprocess\n",
        "from utils.satellite_util import SatelliteUtil\n",
        "from utils.weather_util import WeatherUtil"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1619777569557
        }
      },
      "id": "c524bfa8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Satellite and Weatther Data"
      ],
      "metadata": {},
      "id": "4b79bc38"
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = CONSTANTS['root_dir']"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1619777570641
        }
      },
      "id": "ade23ec6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load satellite data  local paths"
      ],
      "metadata": {},
      "id": "1b807123"
    },
    {
      "cell_type": "code",
      "source": [
        "sat_links = pd.read_csv(os.path.join(CONSTANTS[\"results_dir\"], \"satellite_paths.csv\"))\n",
        "sat_links[\"fileExist\"] = sat_links.filePath.apply(os.path.exists)\n",
        "sat_links.head()\n",
        "\n",
        "# TODO: Check fileExist is True for all rows and raise error  "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1619777571933
        }
      },
      "id": "32059f2a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### List of weather parameter used in model training"
      ],
      "metadata": {},
      "id": "b77e931e"
    },
    {
      "cell_type": "code",
      "source": [
        "weather_parms = [\n",
        "    'airTempMax-F', \n",
        "    'airTempMin-F', \n",
        "    'cloudCover-%', \n",
        "    'dewPoint-F', \n",
        "    'dewPointMax-F', \n",
        "    'dewPointMin-F', \n",
        "    'iceAccPeriod-in', \n",
        "    'liquidAccPeriod-in', \n",
        "    'longWaveRadiationAvg-W/m^2', \n",
        "    'petPeriod-in', \n",
        "    'precipitation-in', \n",
        "    'relativeHumidity-%', \n",
        "    'relativeHumidityMax-%', \n",
        "    'relativeHumidityMin-%', \n",
        "    'shortWaveRadiationAvg-W/m^2', \n",
        "    'snowAccPeriod-in', \n",
        "    'sunshineDuration-hours', \n",
        "    'temperature-F', \n",
        "    'windSpeed-mph', \n",
        "    'windSpeed2mAvg-mph', \n",
        "    'windSpeed2mMax-mph', \n",
        "    'windSpeed2mMin-mph', \n",
        "    'windSpeedMax-mph', \n",
        "    'windSpeedMin-mph'\n",
        "]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1619777572980
        }
      },
      "id": "fa708a63"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Train and Validation sets"
      ],
      "metadata": {},
      "id": "ff9e8ac4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine satellite file paths and weather file per boundary\n",
        "\n",
        "trainval = (\n",
        "    sat_links.drop_duplicates([\"boundaryId\", \"fileExist\"])\n",
        "    .groupby([\"boundaryId\"])[\"fileExist\"]\n",
        "    .agg({\"count\"})\n",
        "    .reset_index()\n",
        "    .query(\"count == 1\")\n",
        "    .drop([\"count\"], axis=1)\n",
        ")\n",
        "\n",
        "\n",
        "# Check for weather file exists or not\n",
        "trainval[\"w_exists\"] = (trainval[\"boundaryId\"] + \"_historical.csv\").apply(lambda x: os.path.join(root_dir, x)).apply(\n",
        "    os.path.exists\n",
        ")\n",
        "\n",
        "trainval = trainval.query(\"w_exists\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1619777573626
        }
      },
      "id": "a72d1211"
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into train and validation sets in 80% and 20% respectively\n",
        "np.random.seed(10)\n",
        "trainval[\"trainval\"] = np.where(\n",
        "    np.random.uniform(0, 1, trainval.shape[0]) < 0.8, \"Train\", \"Val\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1619777574362
        }
      },
      "id": "4b06465f"
    },
    {
      "cell_type": "code",
      "source": [
        "print(trainval)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1619777575097
        }
      },
      "id": "e119f3c2-6612-40ae-9062-945770500634"
    },
    {
      "cell_type": "code",
      "source": [
        "# get mean and standard deviation of training data weather parameters for normalization\r\n",
        "w_stats = pd.concat(\r\n",
        "    [\r\n",
        "        pd.read_csv(os.path.join(root_dir, x + \"_historical.csv\"))\r\n",
        "        for x in trainval.query('trainval == \"Train\"').boundaryId.values\r\n",
        "    ],\r\n",
        "    axis=0,\r\n",
        ")[weather_parms].agg({\"mean\", \"std\"})"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1619777576015
        }
      },
      "id": "d39aabc5-d616-4cb6-8b3a-e8f1942a95b9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get weather statistics for Normalization"
      ],
      "metadata": {},
      "id": "9e574379"
    },
    {
      "cell_type": "code",
      "source": [
        "# get mean and standard deviation of training data weather parameters for normalization\n",
        "w_stats = pd.concat(\n",
        "    [\n",
        "        pd.read_csv(os.path.join(root_dir, x + \"_historical.csv\"))\n",
        "        for x in trainval.query('trainval == \"Train\"').boundaryId.values\n",
        "    ],\n",
        "    axis=0,\n",
        ")[weather_parms].agg({\"mean\", \"std\"})\n",
        "\n",
        "\n",
        "weather_mean = w_stats.filter(like=\"mean\", axis=0)[weather_parms].values\n",
        "weather_std = w_stats.filter(like=\"std\", axis=0)[weather_parms].values\n",
        "\n",
        "# Save weather parameters normalization stats\n",
        "os.makedirs(os.path.dirname(CONSTANTS[\"w_pkl\"]), exist_ok=True)\n",
        "with open(CONSTANTS[\"w_pkl\"], \"wb+\") as f:\n",
        "    pickle.dump([weather_parms, weather_mean, weather_std], f)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1619777576854
        }
      },
      "id": "8bffee31"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ARD(boundaryId):\n",
        "    # function for preparing Analysis Ready Dataset\n",
        "    # intended for use in _2_build_model.py\n",
        "    \n",
        "    boundary_id_sat_links = sat_links.query(\n",
        "        'boundaryId == @boundaryId'\n",
        "    )\n",
        "     \n",
        "    # in reading w_df, if error occurs with farm_code, change it to field_id\n",
        "    w_df = pd.read_csv(os.path.join(root_dir, boundaryId + \"_historical.csv\"))\n",
        "    \n",
        "    da_pc = ard_preprocess(\n",
        "        sat_file_links=boundary_id_sat_links,\n",
        "        w_df=w_df,\n",
        "        sat_res_x=2,\n",
        "        var_name=CONSTANTS[\"var_name\"],\n",
        "        interp_date_start=CONSTANTS[\"interp_date_start\"],\n",
        "        interp_date_end=CONSTANTS[\"interp_date_end\"],\n",
        "        w_parms=weather_parms,\n",
        "        input_days=CONSTANTS[\"input_days\"],\n",
        "        output_days=CONSTANTS[\"output_days\"],\n",
        "        ref_tm=CONSTANTS[\"ref_tm_model\"],\n",
        "        w_mn=weather_mean,\n",
        "        w_sd=weather_std,\n",
        "    )\n",
        "    return da_pc.query(\n",
        "        \"nan_input_evi and nan_input_w and nan_output_evi and nan_output_w and input_evi_le1 and output_evi_le1\"\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1619777577692
        }
      },
      "id": "f5349177"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Analysis Ready Dataset"
      ],
      "metadata": {},
      "id": "ab2a25fe"
    },
    {
      "cell_type": "code",
      "source": [
        "# Get analysis ready dataset\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "with ThreadPoolExecutor(max_workers=100) as executor:\n",
        "    ards_fetch = [executor.submit(get_ARD, x) for x in trainval.boundaryId.values]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1619777741570
        }
      },
      "id": "2d344aba"
    },
    {
      "cell_type": "code",
      "source": [
        "ards_fetch[0].result()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1619777742280
        }
      },
      "id": "0df6a9f5"
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.concat(\n",
        "    [\n",
        "        ards_fetch[x]\n",
        "        .result()\n",
        "        .assign(\n",
        "            boundary_code=trainval.boundaryId.values[x], trainval=trainval.trainval.values[x]\n",
        "        )\n",
        "        for x in range(len(trainval.boundaryId.values))\n",
        "        if ards_fetch[x].exception() == None\n",
        "    ],\n",
        "    axis=0,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1619777743189
        }
      },
      "id": "d2a2100b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Data Preparation"
      ],
      "metadata": {},
      "id": "f4dbd376"
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = data.query('trainval == \"Train\"')\n",
        "data_val = data.query('trainval == \"Val\"')\n",
        "\n",
        "# Prepare train and validation tensors\n",
        "# converting list variables in ARD DataFrame to numpy array (tensors)\n",
        "X_train = [\n",
        "    np.array(data_train.input_evi.to_list()),\n",
        "    np.array(data_train.input_weather.to_list()),\n",
        "    np.array(data_train.forecast_weather.to_list()),\n",
        "]\n",
        "Y_train = np.array(data_train.output_evi.to_list())\n",
        "\n",
        "\n",
        "X_val = [\n",
        "    np.array(data_val.input_evi.to_list()),\n",
        "    np.array(data_val.input_weather.to_list()),\n",
        "    np.array(data_val.forecast_weather.to_list()),\n",
        "]\n",
        "Y_val = np.array(data_val.output_evi.to_list())\n",
        "\n",
        "# Save Analysis Ready Dataset (ARD)\n",
        "os.makedirs(os.path.dirname(CONSTANTS[\"ardpkl\"]), exist_ok=True)\n",
        "with open(CONSTANTS[\"ardpkl\"], \"wb\") as f:\n",
        "    pickle.dump(da_fin, f)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "aa5cacc4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Architecture"
      ],
      "metadata": {},
      "id": "2659bae3"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(input_weather, x, y, z):\n",
        "    \"\"\"\n",
        "    Model architecture\n",
        "    \"\"\"\n",
        "    # intended for use in _2_build_model.py\n",
        "    # Define the tensors for the three input images\n",
        "    evi_input = keras.Input((CONSTANTS[\"input_days\"], 1), name=\"evi_input\")\n",
        "    weather_input = keras.Input(\n",
        "        (CONSTANTS[\"input_days\"], input_weather), name=\"weather_input\"\n",
        "    )\n",
        "    forecast_input = keras.Input(\n",
        "        (CONSTANTS[\"output_days\"], input_weather), name=\"forecast_input\"\n",
        "    )\n",
        "\n",
        "    dense_1 = keras.layers.LSTM(\n",
        "        x, activation=\"relu\", name=\"DeNse_1\", dropout=0.1, recurrent_dropout=0.1\n",
        "    )(evi_input)\n",
        "    dense_2 = keras.layers.LSTM(\n",
        "        y, activation=\"relu\", name=\"DeNse_2\", dropout=0.1, recurrent_dropout=0.1\n",
        "    )(weather_input)\n",
        "    dense_3 = keras.layers.LSTM(\n",
        "        z,\n",
        "        activation=\"relu\",\n",
        "        name=\"lstm_1\",\n",
        "        return_sequences=True,\n",
        "        dropout=0.1,\n",
        "        recurrent_dropout=0.1,\n",
        "    )(forecast_input)\n",
        "\n",
        "    dense_12 = keras.layers.concatenate(axis=-1, inputs=[dense_1, dense_2])\n",
        "    dense_12_1 = keras.layers.RepeatVector(10)(dense_12)\n",
        "    dense_123 = keras.layers.concatenate(axis=-1, inputs=[dense_12_1, dense_3])\n",
        "    prediction = keras.layers.LSTM(\n",
        "        1, activation=\"relu\", name=\"lstm_2\", return_sequences=True\n",
        "    )(dense_123)\n",
        "    # Connect the inputs with the outputs\n",
        "    finnet = keras.Model(\n",
        "        inputs=[evi_input, weather_input, forecast_input], outputs=prediction\n",
        "    )\n",
        "    # return the model\n",
        "    return finnet"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "4afb7d5b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {},
      "id": "32383d0b"
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model(len(w_parms), 100, 100, 100)\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
        "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"mse\"])\n",
        "# Model run\n",
        "training_history = model.fit(\n",
        "    X_train,\n",
        "    Y_train,\n",
        "    epochs=20,\n",
        "    verbose=0,\n",
        "    validation_data=(X_val, Y_val),\n",
        "    callbacks=[],\n",
        "    batch_size=1000,\n",
        ")\n",
        "val_pred = model.predict(X_val)\n",
        "# Save model to h5 format\n",
        "tf.keras.models.save_model(\n",
        "    model, filepath= CONSTANTS[\"modelh5\"], save_format=\"h5\", overwrite=True\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "84c0342a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization"
      ],
      "metadata": {},
      "id": "9eb703a6"
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize model error as function of forecast days\n",
        "err_pred = Y_val[:, :, 0] - val_pred[:, :, 0]\n",
        "err_base = -Y_val[:, :, 0] + X_val[0][:, -1, 0][:, np.newaxis]\n",
        "df_err_mn = pd.DataFrame(\n",
        "    {\n",
        "        \"Day_i_vs_Day_0\": np.sqrt(np.mean(err_base ** 2, axis=0)),\n",
        "        \"Predicted_vs_Actual\": np.sqrt(np.mean(err_pred ** 2, axis=0)),\n",
        "        \"Day\": 1 + np.arange(CONSTANTS[\"output_days\"]),\n",
        "    }\n",
        ").set_index([\"Day\"])\n",
        "df_err_mn.plot()\n",
        "plt.suptitle(\n",
        "    \"RMSE plot comparing Day i to Day 0 vs Day i to ANN model prediction\\n Validation RMSE: \"\n",
        "    + str(np.round(np.sqrt(training_history.history[\"val_mse\"][-1]), 4))\n",
        ")\n",
        "plt.savefig(CONSTANTS[\"model_result_png\"])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "16dc4ab4"
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3-azureml"
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}